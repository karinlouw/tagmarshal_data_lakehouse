# AWS execution configuration (conceptual — used for parity and documentation)
# In AWS you typically won't bake secrets here; IAM roles should provide auth.

# Environment
TM_ENV=aws

# =============================================================================
# BUCKET LAYOUT (S3)
# =============================================================================
#
# tm-lakehouse-landing-zone/        ← Raw data lands here
# tm-lakehouse-source-store/        ← Silver: cleaned, conformed
#   └── warehouse/silver/
# tm-lakehouse-serve/               ← Gold: analytics-ready
#   └── warehouse/gold/
# tm-lakehouse-quarantine/          ← Invalid data
# tm-lakehouse-observability/       ← Run logs & artifacts
#
# =============================================================================

#Todo: remove lakehouse from name
TM_BUCKET_LANDING=tm-lakehouse-landing-zone
TM_BUCKET_SOURCE=tm-lakehouse-source-store
TM_BUCKET_SERVE=tm-lakehouse-serve
TM_BUCKET_QUARANTINE=tm-lakehouse-quarantine
TM_BUCKET_OBSERVABILITY=tm-lakehouse-observability

# AWS region
TM_S3_REGION=us-east-1

# Iceberg catalog - Glue for AWS
TM_ICEBERG_CATALOG_TYPE=glue
TM_ICEBERG_WAREHOUSE_SILVER=s3://tm-lakehouse-source-store/warehouse
TM_ICEBERG_WAREHOUSE_GOLD=s3://tm-lakehouse-serve/warehouse

# Todo: What is meant by namespaces?
# Namespaces (Iceberg schemas)
TM_DB_SILVER=silver
TM_DB_GOLD=gold

#Todo: what is the default?
# Observability output prefix
TM_OBS_PREFIX=

# PostgreSQL credentials (for Airflow metadata DB)
# In AWS, these would typically come from RDS or managed Postgres
TM_POSTGRES_USER=airflow
TM_POSTGRES_PASSWORD=CHANGE_ME_IN_PRODUCTION
TM_POSTGRES_DB=airflow

# Airflow admin user credentials
# In AWS, consider using IAM/SSO integration instead
TM_AIRFLOW_ADMIN_USERNAME=admin
TM_AIRFLOW_ADMIN_PASSWORD=CHANGE_ME_IN_PRODUCTION
TM_AIRFLOW_ADMIN_EMAIL=admin@example.com

# =============================================================================
# SPARK CONFIGURATION (AWS - optimized for EMR/Glue clusters)
# =============================================================================
# These settings are tuned for AWS cluster execution with more resources.
#
# Spark master mode:
#   - "yarn": AWS EMR (recommended for production)
#   - "local[*]": Use all available cores (for single-node testing)
#
TM_SPARK_MASTER=yarn
TM_SPARK_DRIVER_MEMORY=4g
TM_SPARK_EXECUTOR_MEMORY=8g
TM_SPARK_SHUFFLE_PARTITIONS=200
TM_SPARK_ADAPTIVE_ENABLED=true
TM_SPARK_UI_ENABLED=true

# Pipeline execution mode:
#   - "sequential": Run jobs one at a time
#   - "parallel": Run all jobs at once (recommended for AWS with abundant resources)
TM_PIPELINE_MODE=parallel

# =============================================================================
# API Configuration (for TagMarshal API)
# =============================================================================
# TM_DATA_SOURCE controls where data comes from:
#   - "file": Read CSV/JSON from S3
#   - "api": Fetch JSON from TagMarshal API
#
TM_DATA_SOURCE=api
TM_API_BASE_URL=https://api.tagmarshal.com
TM_API_KEY=STORE_IN_SECRETS_MANAGER
TM_API_TIMEOUT=30
