# AWS execution configuration
# This file contains ONLY AWS-specific overrides.
# Shared defaults are in base.env (loaded first).
#
# In AWS you typically won't bake secrets here; IAM roles should provide auth.

# =============================================================================
# ENVIRONMENT IDENTIFIER
# =============================================================================

TM_ENV=aws

# =============================================================================
# S3 CONFIGURATION (no endpoint needed for AWS - uses default)
# =============================================================================
# Todo: remove lakehouse from name

# No TM_S3_ENDPOINT - uses default AWS S3
# No TM_S3_ACCESS_KEY/SECRET_KEY - uses IAM role authentication
TM_S3_FORCE_PATH_STYLE=false

# =============================================================================
# ICEBERG CATALOG (Glue for AWS)
# =============================================================================
# Todo: What is meant by namespaces?

TM_ICEBERG_CATALOG_TYPE=glue
# No TM_ICEBERG_REST_URI - Glue uses AWS APIs directly

# =============================================================================
# POSTGRESQL (for Airflow metadata DB)
# =============================================================================
# In AWS, these would typically come from RDS or managed Postgres
# Todo: what is the default?

TM_POSTGRES_USER=airflow
TM_POSTGRES_PASSWORD=CHANGE_ME_IN_PRODUCTION
TM_POSTGRES_DB=airflow

# =============================================================================
# AIRFLOW ADMIN CREDENTIALS
# =============================================================================
# In AWS, consider using IAM/SSO integration instead

TM_AIRFLOW_ADMIN_USERNAME=admin
TM_AIRFLOW_ADMIN_PASSWORD=CHANGE_ME_IN_PRODUCTION
TM_AIRFLOW_ADMIN_EMAIL=admin@example.com

# =============================================================================
# SPARK CONFIGURATION (AWS - optimized for EMR/Glue clusters)
# =============================================================================
# These settings are tuned for AWS cluster execution with more resources.
#
# Spark master mode:
#   - "yarn": AWS EMR (recommended for production)
#   - "local[*]": Use all available cores (for single-node testing)

TM_SPARK_MASTER=yarn
TM_SPARK_DRIVER_MEMORY=4g
TM_SPARK_EXECUTOR_MEMORY=8g
TM_SPARK_SHUFFLE_PARTITIONS=200
TM_SPARK_ADAPTIVE_ENABLED=true
TM_SPARK_UI_ENABLED=true

# =============================================================================
# PIPELINE EXECUTION MODE
# =============================================================================
# Pipeline execution mode:
#   - "sequential": Run jobs one at a time
#   - "parallel": Run all jobs at once (recommended for AWS with abundant resources)

TM_PIPELINE_MODE=parallel

# =============================================================================
# API CONFIGURATION OVERRIDE
# =============================================================================
# In AWS, data comes from the API instead of local files

TM_DATA_SOURCE=api
TM_API_KEY=STORE_IN_SECRETS_MANAGER
