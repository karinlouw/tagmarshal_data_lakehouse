# Local development configuration (Docker Compose)
# This file is intentionally tracked so juniors can learn by reading defaults.

# Environment
TM_ENV=local

# =============================================================================
# BUCKET LAYOUT (MinIO / S3)
# =============================================================================
#
# tm-lakehouse-landing-zone/                    ← Raw data lands here
#   └── course_id=<course>/ingest_date=<date>/<filename>.csv
#
# tm-lakehouse-source-store/                    ← Silver: cleaned, conformed
#   └── warehouse/silver/fact_telemetry_event/
#       ├── metadata/
#       └── data/course_id=X/event_date=Y/
#
# tm-lakehouse-serve/                           ← Gold: analytics-ready
#   └── warehouse/gold/
#       ├── pace_summary_by_round/
#       ├── signal_quality_rounds/
#       └── device_health_errors/
#
# tm-lakehouse-quarantine/                      ← Invalid data
#   └── silver/course_id=X/ingest_date=Y/
#
# tm-lakehouse-observability/                   ← Run logs & artifacts
#   ├── bronze/run_id=<id>.json
#   └── silver/course_id=X/ingest_date=Y/
#
# =============================================================================

TM_BUCKET_LANDING=tm-lakehouse-landing-zone
TM_BUCKET_SOURCE=tm-lakehouse-source-store
TM_BUCKET_SERVE=tm-lakehouse-serve
TM_BUCKET_QUARANTINE=tm-lakehouse-quarantine
TM_BUCKET_OBSERVABILITY=tm-lakehouse-observability

# S3-compatible endpoint (MinIO)
TM_S3_ENDPOINT=http://minio:9000
TM_S3_REGION=us-east-1
TM_S3_ACCESS_KEY=minioadmin
TM_S3_SECRET_KEY=minioadmin
TM_S3_FORCE_PATH_STYLE=true

# Todo: learn more about the ice berg schemas
# Iceberg catalog configuration
TM_ICEBERG_CATALOG_TYPE=rest
TM_ICEBERG_REST_URI=http://iceberg-rest:8181
TM_ICEBERG_WAREHOUSE_SILVER=s3://tm-lakehouse-source-store/warehouse
TM_ICEBERG_WAREHOUSE_GOLD=s3://tm-lakehouse-serve/warehouse

# Namespaces (Iceberg schemas)
TM_DB_SILVER=silver
TM_DB_GOLD=gold

# Observability output prefix (empty = write directly to bucket root)
TM_OBS_PREFIX=

#Todo: local input like data source?
# Default local input directory (mounted into containers)
TM_LOCAL_INPUT_DIR=/opt/tagmarshal/input

#Todo: investigate metadata db
# PostgreSQL credentials (for Airflow metadata DB)
TM_POSTGRES_USER=airflow
TM_POSTGRES_PASSWORD=airflow
TM_POSTGRES_DB=airflow

# Airflow admin user credentials
TM_AIRFLOW_ADMIN_USERNAME=admin
TM_AIRFLOW_ADMIN_PASSWORD=admin
TM_AIRFLOW_ADMIN_EMAIL=admin@example.com

# =============================================================================
# SPARK CONFIGURATION (local development - conservative settings)
# =============================================================================
# These settings are tuned for laptop development (limited CPU/memory).
# AWS environment has more aggressive settings for cluster execution.
#
# Spark master mode:
#   - "local[N]": Run locally with N threads (good for laptops)
#   - "spark://host:7077": Standalone cluster (for distributed local testing)
#   - "yarn": AWS EMR (production)
#
TM_SPARK_MASTER=local[2]
TM_SPARK_DRIVER_MEMORY=1g
TM_SPARK_EXECUTOR_MEMORY=1g
TM_SPARK_SHUFFLE_PARTITIONS=8
TM_SPARK_ADAPTIVE_ENABLED=true
TM_SPARK_UI_ENABLED=false

# Pipeline execution mode:
#   - "sequential": Run jobs one at a time (faster on laptops due to less contention)
#   - "parallel": Run all jobs at once (faster on clusters with lots of resources)
# For local dev, sequential is often faster: 6 jobs × 30s = 3 min
# vs parallel: 6 jobs competing = 5-6 min each
TM_PIPELINE_MODE=sequential

# =============================================================================
# API Configuration (for future use with TagMarshal API)
# =============================================================================
# When you get API access, fill in these values and set TM_DATA_SOURCE=api
#
# TM_DATA_SOURCE controls where data comes from:
#   - "file" (default): Read CSV/JSON from local files
#   - "api": Fetch JSON from TagMarshal API
#
TM_DATA_SOURCE=file
TM_API_BASE_URL=https://api.tagmarshal.com
TM_API_KEY=
TM_API_TIMEOUT=30
