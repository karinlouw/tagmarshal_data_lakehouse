name: tagmarshal-lakehouse

# =============================================================================
# CONFIGURATION
# =============================================================================
# Environment variables are loaded from:
# 1. config/base.env - Shared defaults (bucket names, schemas, etc.)
# 2. config/local.env - Local-specific overrides (MinIO endpoints, credentials)
#
# For AWS deployment, use: TM_ENV=aws docker-compose up
# which would load config/aws.env instead of config/local.env
# =============================================================================

# =============================================================================
# STORAGE LAYER (S3-compatible object storage)
# =============================================================================

services:
  # MinIO: Local S3-compatible storage (replaces AWS S3 for local dev)
  # Port 9000 = API, Port 9001 = Web console
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${TM_S3_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${TM_S3_SECRET_KEY:-minioadmin}
    ports:
      - "9000:9000" # S3 API
      - "9001:9001" # Web UI
    volumes:
      - minio_data:/data # Persistent storage for buckets
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/ready" ]
      interval: 5s
      timeout: 3s
      retries: 20

  # MinIO Client: Creates buckets on startup (one-time setup)
  # Runs after MinIO is healthy, then exits
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      TM_S3_ACCESS_KEY: ${TM_S3_ACCESS_KEY:-minioadmin}
      TM_S3_SECRET_KEY: ${TM_S3_SECRET_KEY:-minioadmin}
      TM_BUCKET_LANDING: ${TM_BUCKET_LANDING:-tm-lakehouse-landing-zone}
      TM_BUCKET_SOURCE: ${TM_BUCKET_SOURCE:-tm-lakehouse-source-store}
      TM_BUCKET_SERVE: ${TM_BUCKET_SERVE:-tm-lakehouse-serve}
      TM_BUCKET_QUARANTINE: ${TM_BUCKET_QUARANTINE:-tm-lakehouse-quarantine}
      TM_BUCKET_OBSERVABILITY: ${TM_BUCKET_OBSERVABILITY:-tm-lakehouse-observability}
    entrypoint: /bin/sh
    command:
      - -c
      - |
        mc alias set local http://minio:9000 $$TM_S3_ACCESS_KEY $$TM_S3_SECRET_KEY &&
        mc mb -p local/$$TM_BUCKET_LANDING || true &&
        mc mb -p local/$$TM_BUCKET_SOURCE || true &&
        mc mb -p local/$$TM_BUCKET_SERVE || true &&
        mc mb -p local/$$TM_BUCKET_QUARANTINE || true &&
        mc mb -p local/$$TM_BUCKET_OBSERVABILITY || true &&
        echo "[SUCCESS] MinIO buckets created:"
        mc ls local/

  # =============================================================================
  # TABLE FORMAT & CATALOG (Apache Iceberg)
  # =============================================================================

  # Iceberg REST Catalog: Manages Iceberg table metadata (schemas, partitions, snapshots)
  # In AWS production, this would be Glue Data Catalog instead
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    depends_on:
      minio-init:
        condition: service_completed_successfully
    environment:
      # Default warehouse location (Silver)
      CATALOG_WAREHOUSE: ${TM_ICEBERG_WAREHOUSE_SILVER:-s3://tm-lakehouse-source-store/warehouse}
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: ${TM_S3_ENDPOINT:-http://minio:9000}
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      AWS_REGION: ${TM_S3_REGION:-us-east-1}
      AWS_ACCESS_KEY_ID: ${TM_S3_ACCESS_KEY:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${TM_S3_SECRET_KEY:-minioadmin}
    ports:
      - "8181:8181" # REST API for catalog operations

  # =============================================================================
  # ETL ENGINE (Apache Spark)
  # =============================================================================

  # Spark Master: Coordinates Spark jobs (Silver ETL runs here)
  # Port 7077 = Master API, Port 8082 = Spark UI
  # Custom image with pre-baked AWS/Iceberg JARs (no runtime downloads!)
  spark:
    build:
      context: ./pipeline/infrastructure/docker/spark
      dockerfile: Dockerfile
    image: tagmarshal-spark:3.5.0
    container_name: spark
    environment:
      SPARK_MASTER_HOST: spark
      SPARK_NO_DAEMONIZE: "true"
    entrypoint: /bin/bash
    command:
      - -c
      - /opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*
    ports:
      - "7077:7077" # Master API
      - "8082:8080" # Spark UI
    volumes:
      - ./pipeline:/opt/tagmarshal/pipeline:ro # Read-only: ETL scripts
      - ./data:/opt/tagmarshal/input:ro # Read-only: Input CSV/JSON files

  # Spark Worker: Executes Spark tasks (optional for local dev)
  # Connects to master to run distributed jobs
  spark-worker:
    build:
      context: ./pipeline/infrastructure/docker/spark
      dockerfile: Dockerfile
    image: tagmarshal-spark:3.5.0
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      SPARK_NO_DAEMONIZE: "true"
    entrypoint: /bin/bash
    command:
      - -c
      - sleep 5 && /opt/spark/sbin/start-worker.sh spark://spark:7077 && tail -f /opt/spark/logs/*
    volumes:
      - ./pipeline:/opt/tagmarshal/pipeline:ro
      - ./data:/opt/tagmarshal/input:ro

  # =============================================================================
  # SQL QUERY ENGINE (Trino)
  # =============================================================================

  # Trino: SQL query engine for Iceberg tables (replaces AWS Athena in local dev)
  # Allows interactive SQL queries against Silver/Gold tables
  trino:
    image: trinodb/trino:455
    container_name: trino
    depends_on:
      - iceberg-rest
    ports:
      - "8081:8080" # Trino UI and API
    volumes:
      - ./pipeline/infrastructure/services/trino/etc:/etc/trino:ro # Catalog config

  # =============================================================================
  # ORCHESTRATION (Apache Airflow)
  # =============================================================================

  # PostgreSQL: Airflow metadata database (stores DAG runs, task history, connections)
  postgres:
    image: postgres:16
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${TM_POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${TM_POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${TM_POSTGRES_DB:-airflow}
    ports:
      - "5432:5432" # PostgreSQL port
    volumes:
      - airflow_pg:/var/lib/postgresql/data # Persistent DB storage
      # Auto-run migrations on first startup (creates ingestion_log table)
      - ./pipeline/infrastructure/database:/docker-entrypoint-initdb.d:ro

  # Airflow: Orchestrates the data pipeline (Bronze → Silver → Gold)
  # Port 8080 = Airflow UI (admin/admin)
  airflow:
    image: apache/airflow:2.10.3-python3.11
    container_name: airflow
    depends_on:
      - postgres
      - spark
      - trino
    env_file:
      - config/base.env
      - config/local.env
    environment:
      # Airflow core config
      AIRFLOW__CORE__EXECUTOR: LocalExecutor # Runs tasks in same container
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${TM_POSTGRES_USER:-airflow}:${TM_POSTGRES_PASSWORD:-airflow}@postgres:5432/${TM_POSTGRES_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      PYTHONWARNINGS: "ignore::FutureWarning,ignore::UserWarning"
      AIRFLOW__WEBSERVER__RATE_LIMIT_ENABLED: "false"
      PYTHONPATH: /opt/tagmarshal/pipeline:/opt/tagmarshal/pipeline/lib # So Airflow can import bronze, silver, and shared libs
      # Airflow admin user (created on first startup)
      TM_AIRFLOW_ADMIN_USERNAME: ${TM_AIRFLOW_ADMIN_USERNAME:-admin}
      TM_AIRFLOW_ADMIN_PASSWORD: ${TM_AIRFLOW_ADMIN_PASSWORD:-admin}
      TM_AIRFLOW_ADMIN_EMAIL: ${TM_AIRFLOW_ADMIN_EMAIL:-admin@example.com}
      # Tagmarshal env vars (passed to DAGs and Spark jobs)
      TM_ENV: ${TM_ENV:-local}
      TM_BUCKET_LANDING: ${TM_BUCKET_LANDING:-tm-lakehouse-landing-zone}
      TM_BUCKET_SOURCE: ${TM_BUCKET_SOURCE:-tm-lakehouse-source-store}
      TM_BUCKET_SERVE: ${TM_BUCKET_SERVE:-tm-lakehouse-serve}
      TM_BUCKET_QUARANTINE: ${TM_BUCKET_QUARANTINE:-tm-lakehouse-quarantine}
      TM_BUCKET_OBSERVABILITY: ${TM_BUCKET_OBSERVABILITY:-tm-lakehouse-observability}
      TM_S3_ENDPOINT: ${TM_S3_ENDPOINT:-http://minio:9000}
      TM_S3_REGION: ${TM_S3_REGION:-us-east-1}
      TM_S3_ACCESS_KEY: ${TM_S3_ACCESS_KEY:-minioadmin}
      TM_S3_SECRET_KEY: ${TM_S3_SECRET_KEY:-minioadmin}
      TM_S3_FORCE_PATH_STYLE: ${TM_S3_FORCE_PATH_STYLE:-true}
      TM_ICEBERG_CATALOG_TYPE: ${TM_ICEBERG_CATALOG_TYPE:-rest}
      TM_ICEBERG_REST_URI: ${TM_ICEBERG_REST_URI:-http://iceberg-rest:8181}
      TM_ICEBERG_WAREHOUSE_SILVER: ${TM_ICEBERG_WAREHOUSE_SILVER:-s3://tm-lakehouse-source-store/warehouse}
      TM_ICEBERG_WAREHOUSE_GOLD: ${TM_ICEBERG_WAREHOUSE_GOLD:-s3://tm-lakehouse-serve/warehouse}
      TM_DB_SILVER: ${TM_DB_SILVER:-silver}
      TM_DB_GOLD: ${TM_DB_GOLD:-gold}
      TM_OBS_PREFIX: ${TM_OBS_PREFIX:-}
      TM_LOCAL_INPUT_DIR: ${TM_LOCAL_INPUT_DIR:-/opt/tagmarshal/input}
      # Spark configuration (environment-specific tuning)
      TM_SPARK_MASTER: ${TM_SPARK_MASTER:-local[2]}
      TM_SPARK_DRIVER_MEMORY: ${TM_SPARK_DRIVER_MEMORY:-1g}
      TM_SPARK_EXECUTOR_MEMORY: ${TM_SPARK_EXECUTOR_MEMORY:-1g}
      TM_SPARK_SHUFFLE_PARTITIONS: ${TM_SPARK_SHUFFLE_PARTITIONS:-8}
      TM_SPARK_ADAPTIVE_ENABLED: ${TM_SPARK_ADAPTIVE_ENABLED:-true}
      TM_SPARK_UI_ENABLED: ${TM_SPARK_UI_ENABLED:-false}
      # Registry database (for ingestion tracking)
      TM_POSTGRES_HOST: postgres
      TM_POSTGRES_PORT: "5432"
      TM_POSTGRES_DB: ${TM_POSTGRES_DB:-airflow}
      TM_POSTGRES_USER: ${TM_POSTGRES_USER:-airflow}
      TM_POSTGRES_PASSWORD: ${TM_POSTGRES_PASSWORD:-airflow}
    volumes:
      - ./pipeline/orchestration/dags:/opt/airflow/dags # DAG definitions
      - ./pipeline/orchestration/logs:/opt/airflow/logs # Task logs
      - ./pipeline/orchestration/plugins:/opt/airflow/plugins
      - ./pipeline/orchestration/requirements.txt:/opt/airflow/requirements.txt:ro
      - ./pipeline:/opt/tagmarshal/pipeline:ro # Pipeline code (ETL, dbt, lib)
      - ./pipeline/gold/logs:/opt/tagmarshal/pipeline/gold/logs # dbt logs (writable)
      - ./pipeline/gold/target:/opt/tagmarshal/pipeline/gold/target # dbt target (writable)
      - ./data:/opt/tagmarshal/input:ro # Input CSV/JSON files
      - /var/run/docker.sock:/var/run/docker.sock # Allows Airflow to exec Docker commands
    ports:
      - "8080:8080" # Airflow UI
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "[INFO] Installing Python dependencies..."
        pip install --no-cache-dir -r /opt/airflow/requirements.txt
        echo "[INFO] Waiting for database..."
        sleep 5
        echo "[INFO] Initializing Airflow database..."
        airflow db init
        echo "[INFO] Creating admin user..."
        airflow users create --username $$TM_AIRFLOW_ADMIN_USERNAME --password $$TM_AIRFLOW_ADMIN_PASSWORD --firstname admin --lastname admin --role Admin --email $$TM_AIRFLOW_ADMIN_EMAIL || true
        echo "[SUCCESS] Airflow initialized. Starting webserver and scheduler..."
        (airflow webserver &) && exec airflow scheduler

  # =============================================================================
  # VISUALIZATION (Apache Superset)
  # =============================================================================

  # Superset: BI dashboard tool (connects to Trino to visualize Gold tables)
  # Port 8088 = Superset UI (admin/admin)
  superset:
    image: apache/superset:3.1.0
    container_name: superset
    depends_on:
      - trino
      - postgres
    environment:
      SUPERSET_SECRET_KEY: ${TM_SUPERSET_SECRET_KEY:-tagmarshal_superset_secret_key_change_me}
      SUPERSET_LOAD_EXAMPLES: "false"
      # Use SQLite for simplicity (PostgreSQL for production)
      SQLALCHEMY_DATABASE_URI: sqlite:////app/superset_home/superset.db
    ports:
      - "8088:8088" # Superset UI
    volumes:
      - superset_home:/app/superset_home # Persistent storage for dashboards
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "[INFO] Installing Trino driver..."
        pip install trino sqlalchemy-trino --quiet
        echo "[INFO] Initializing Superset database..."
        superset db upgrade
        echo "[INFO] Creating admin user..."
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true
        echo "[INFO] Initializing Superset..."
        superset init
        echo "[SUCCESS] Superset ready. Starting server..."
        superset run -h 0.0.0.0 -p 8088 --with-threads --reload

# =============================================================================
# PERSISTENT VOLUMES (Data survives container restarts)
# =============================================================================

volumes:
  minio_data: # MinIO bucket data
  airflow_pg: # Airflow metadata database
  superset_home: # Superset dashboards and config
