# TagMarshal Data Lakehouse

A **local-first, AWS-ready** data lakehouse for golf course round data. Built with modern data engineering best practices: medallion architecture, Apache Iceberg tables, and full observability.

> **Note:** This lakehouse processes **round strings** (pre-processed data from TagMarshal's system), not raw GPS telemetry. The raw GPS pings have already been processed into structured rounds with pace calculations, hole assignments, and section tracking.

## ğŸš€ Quick Start

### 1. Start the stack
```bash
just up
```

### 2. Ingest data (Bronze layer)
```bash
just bronze-upload course_id=<course_id> input=<path_to_file>
```

### 3. Transform to Silver
```bash
just silver course_id=<course_id> ingest_date=YYYY-MM-DD
```

### 4. Build analytics (Gold layer)
```bash
just gold
```

### 5. View dashboard
```bash
just dashboard
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ config/              # Environment configs (local.env, aws.env)
â”œâ”€â”€ data/                # Sample CSV/JSON files (gitignored)
â”œâ”€â”€ dashboard/           # Streamlit data quality dashboard
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ learning/        # Learning guides (pipeline, dbt, airflow, etc.)
â”‚   â”œâ”€â”€ project/         # Project overview and architecture
â”‚   â””â”€â”€ proposals/       # Client proposals
â”œâ”€â”€ infrastructure/      # Infrastructure configuration
â”‚   â”œâ”€â”€ database/        # Database migrations
â”‚   â””â”€â”€ services/        # Docker service configs (Trino, etc.)
â”œâ”€â”€ jobs/                # Spark ETL jobs
â”‚   â””â”€â”€ spark/           # Silver ETL transformation
â”œâ”€â”€ monitoring/          # Monitoring & alerting configs
â”œâ”€â”€ notebooks/           # Jupyter notebooks for exploration
â”œâ”€â”€ orchestration/       # Airflow DAGs and config
â”‚   â””â”€â”€ airflow/         # DAG definitions
â”œâ”€â”€ queries/             # SQL exploration queries
â”‚   â”œâ”€â”€ exploration/     # Dashboard and analysis queries
â”‚   â””â”€â”€ examples/        # Example queries
â”œâ”€â”€ schemas/             # Table schemas and DDLs
â”‚   â”œâ”€â”€ bronze/          # Bronze layer schemas
â”‚   â”œâ”€â”€ silver/          # Silver layer schemas
â”‚   â””â”€â”€ gold/            # Gold layer schemas
â”œâ”€â”€ scripts/             # Utility scripts (backfill, etc.)
â”œâ”€â”€ tests/               # Integration and data quality tests
â”‚   â”œâ”€â”€ integration/     # End-to-end pipeline tests
â”‚   â”œâ”€â”€ data_quality/    # Data quality validation tests
â”‚   â””â”€â”€ fixtures/        # Test data files
â”œâ”€â”€ transform/           # dbt project for Gold layer
â”‚   â””â”€â”€ dbt_project/     # dbt models and config
â””â”€â”€ validations/         # Data validation rules
    â”œâ”€â”€ rules/           # Validation rule definitions
    â””â”€â”€ thresholds/      # Quality thresholds
```

## ğŸ—ï¸ Architecture

**Tech Stack:**
- **Storage**: MinIO (local) / S3 (AWS)
- **Table Format**: Apache Iceberg
- **ETL**: Apache Spark
- **SQL Engine**: Trino (local) / Athena (AWS)
- **Orchestration**: Apache Airflow
- **Transforms**: dbt

**Data Flow:**
```
CSV/JSON â†’ Bronze (raw) â†’ Silver (cleaned) â†’ Gold (analytics)
```

## ğŸ“š Documentation

- **Getting Started**: `docs/runbook_local_dev.md`
- **Pipeline Walkthrough**: `docs/learning/pipeline_walkthrough.md`
- **Command Reference**: `docs/learning/command_reference.md`
- **Project Overview**: `docs/project/PROJECT_OVERVIEW.md`
- **AWS Migration**: `docs/aws_cutover.md`

## ğŸ”§ Configuration

### Local Development
Edit `config/local.env` to change local settings.

### Switch to API Source
When you get API access, update `config/local.env`:
```bash
TM_DATA_SOURCE=api
TM_API_KEY=your-api-key-here
```

## ğŸ’¡ For Junior Developers

This project is designed to be **simple and clear**:
- Each folder has a `README.md` explaining its purpose
- Code is well-commented and straightforward
- Learning guides in `docs/learning/` explain each component
- Use `just` commands (see `docs/learning/command_reference.md`) instead of complex Docker commands

## ğŸ› ï¸ Common Tasks

```bash
# View all available commands
just --list

# Check service status
just status

# Run data quality checks
just dq

# Query data with Trino
just trino-query "SELECT * FROM iceberg.silver.fact_telemetry_event LIMIT 10"
```

## ğŸ“Š Dashboard

The Streamlit dashboard shows data quality metrics and insights:
```bash
just dashboard
```

Access at http://localhost:8501
