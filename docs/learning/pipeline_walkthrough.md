# Pipeline Walkthrough - Step by Step

> **Working Document**: We update this as we run each pipeline step together.
> 
> **Goal**: Understand exactly what happens at each stage, which files run, and how data flows through the system.

---

## Quick Reference

| Step | Command | What Happens | Status |
|------|---------|--------------|--------|
| 1 | `just up` | Start all Docker services | âœ… |
| 2 | `just bronze-upload-all` | Upload CSV/JSON to Bronze (MinIO) | âœ… |
| 3 | `just silver-all` | Transform Bronze â†’ Silver (Iceberg) | âœ… |
| 4 | `just gold` | Build analytics tables with dbt | âœ… |
| 5 | `just trino` | Query results in Trino | âœ… |

**Full Pipeline (all steps at once):**
```bash
just pipeline-all
```

---

## Step 1: Start the Docker Stack âœ…

### Command
```bash
just up
```

### What It Does
Starts 7 Docker containers that make up our lakehouse:

| Container | Purpose | Port |
|-----------|---------|------|
| `minio` | S3-compatible storage (our "data lake") | 9000, 9001 |
| `minio-init` | Creates buckets on startup | (exits after setup) |
| `iceberg-rest` | Table catalog - tracks where data lives | 8181 |
| `spark` | ETL engine for transformations | 7077, 8082 |
| `spark-worker` | Spark worker node | - |
| `trino` | SQL query engine | 8081 |
| `airflow` | Orchestration - runs our DAGs | 8080 |
| `airflow-postgres` | Airflow's metadata database | 5432 |

### Files Involved
```
docker-compose.yml          # Defines all containers
config/local.env            # Environment variables
```

### How to Verify
```bash
# Check all containers are running
docker ps

# Check Airflow is ready
curl -s http://localhost:8080/health
```

### URLs to Bookmark
- **Airflow**: http://localhost:8080 (admin/admin)
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Trino**: http://localhost:8081
- **Spark Master**: http://localhost:8082

---

## Step 2: Bronze Layer - Upload Data âœ…

> **Status**: âœ… Complete

### Commands

**Option A: Upload a single file**
```bash
just bronze-upload <course_id> <file_path>

# Examples:
just bronze-upload indiancreek data/indiancreek.rounds.csv
just bronze-upload indiancreek data/indiancreek.rounds_json.json
```

**Option B: Upload ALL files in data/ folder**
```bash
just bronze-upload-all

# With custom date:
just bronze-upload-all 2026-01-03
```

This uploads all CSV and JSON files at once:

| File | Size | Course ID |
|------|------|-----------|
| americanfalls.rounds.csv | 13MB | americanfalls |
| bradshawfarmgc.rounds_0601_0715.csv | 60MB | bradshawfarmgc |
| bradshawfarmgc.rounds_0716_0831.csv | 66MB | bradshawfarmgc |
| erinhills.rounds.csv | 23MB | erinhills |
| indiancreek.rounds.csv | 10MB | indiancreek |
| indiancreek.rounds_json.json | 38MB | indiancreek |
| pinehurst4.rounds.csv | 22MB | pinehurst4 |

> **Note**: Multiple files with the same course_id (like bradshawfarmgc) are stored separately - the filename is part of the S3 key, so they don't overwrite each other.

### What It Does
1. Validates the file (checks structure, counts rows)
2. Uploads to MinIO bucket: `tm-lakehouse-landing-zone`
3. Records the upload in the ingestion registry (PostgreSQL)

### View the Ingestion Registry
```bash
# View today's ingestion status
just ingestion-status

# View status for a specific date
just ingestion-status 2026-01-03

# Clear registry for a date (if you need to re-run)
just registry-clear 2026-01-03
```

> **Note**: The registry is auto-initialized when PostgreSQL first starts (via `docker-entrypoint-initdb.d`). On existing setups, run `just registry-init` once.

### Files That Run
```
jobs/spark/lib/tm_lakehouse/bronze.py   # Main logic (186 lines)
  â”œâ”€â”€ detect_file_format()              # CSV or JSON? (line 33)
  â”œâ”€â”€ validate_csv_header()             # Check CSV headers (line 53)
  â”œâ”€â”€ validate_json_structure()         # Check JSON structure (line 66)
  â””â”€â”€ upload_file_to_bronze()           # Upload to MinIO (line 118)

orchestration/airflow/dags/bronze_ingest_dag.py  # Airflow DAG (149 lines)
  â””â”€â”€ bronze_upload_task()              # Main task function (line 24)
```

### Data Flow
```
Local File (data/indiancreek.rounds.csv)
    â†“
Airflow DAG triggered (bronze_ingest)
    â†“
Validation (check _id, course, locations[0].startTime)
    â†“
MinIO: s3://tm-lakehouse-landing-zone/course_id=indiancreek/ingest_date=2026-01-03/indiancreek.rounds.csv
    â†“
Registry: PostgreSQL ingestion_log table
```

### Our Upload Result
```
course_id=indiancreek/ingest_date=2026-01-03/indiancreek.rounds.csv (10,975,831 bytes)
```

### How to Verify
```bash
# Check MinIO for uploaded file (via Airflow container)
docker exec airflow python3 -c "
import boto3
s3 = boto3.client('s3', endpoint_url='http://minio:9000',
    aws_access_key_id='minioadmin', aws_secret_access_key='minioadmin')
for obj in s3.list_objects_v2(Bucket='tm-lakehouse-landing-zone', 
    Prefix='course_id=indiancreek').get('Contents', []):
    print(f'{obj[\"Key\"]} ({obj[\"Size\"]:,} bytes)')
"

# Check pipeline status
just status
```

### Bug We Fixed
The `just bronze-upload` command was passing `data/file.csv` directly, but inside the Docker container the data is mounted at `/opt/tagmarshal/input/`. Fixed by converting the path automatically.

---

## Understanding: How Deduplication Works

### The Challenge
We have files like:
- `bradshawfarmgc.rounds_0601_0715.csv` (June data)
- `bradshawfarmgc.rounds_0716_0831.csv` (July data)

Both are the **same course** (`Bradshaw Farms`) but different time periods. How do we:
1. Avoid duplicates when re-running backfills?
2. Allow daily incremental uploads?
3. Keep historical data intact?

### The Solution: Three Layers of Protection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: BRONZE (Landing Zone)                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Files stored by: course_id + ingest_date + filename        â”‚
â”‚                                                             â”‚
â”‚  s3://landing-zone/                                         â”‚
â”‚    course_id=bradshawfarmgc/                                â”‚
â”‚      ingest_date=2026-01-03/                                â”‚
â”‚        bradshawfarmgc.rounds_0601_0715.csv  âœ“               â”‚
â”‚        bradshawfarmgc.rounds_0716_0831.csv  âœ“               â”‚
â”‚                                                             â”‚
â”‚  â†’ Same file uploaded twice? SKIPPED (idempotent)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: INGESTION REGISTRY (PostgreSQL)                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Tracks: filename + ingest_date + stage + file_hash         â”‚
â”‚                                                             â”‚
â”‚  â†’ Already ingested? SKIPPED before upload even starts      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: SILVER (Iceberg Table)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Deduplication: round_id + fix_timestamp                    â”‚
â”‚                                                             â”‚
â”‚  Before APPEND:                                             â”‚
â”‚    DELETE FROM silver.fact_telemetry_event                  â”‚
â”‚    WHERE course_id = 'bradshawfarmgc'                       â”‚
â”‚      AND ingest_date = '2026-01-03'                         â”‚
â”‚                                                             â”‚
â”‚  â†’ Re-running Silver ETL? Old data DELETED first, then new  â”‚
â”‚    data APPENDED (idempotent)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Points

1. **Single Table for All Courses**: We DON'T create separate tables per course. All data goes into `silver.fact_telemetry_event`, partitioned by `course_id` and `event_date`.

2. **Natural Deduplication**: Each round has a unique `_id` from MongoDB. Within a round, each location record has a unique `fix_timestamp`. The combination `(round_id, fix_timestamp)` is unique.

# Todo: make sure to check row counts and file size to determine file already uploaded
3. **Idempotent Re-runs**: You can safely run:
   - `just bronze-upload-all` multiple times â†’ files already uploaded are skipped
   - `just silver-all` multiple times â†’ old data deleted before new data appended

4. **Backfill + Daily Updates**:
   ```bash
   # Initial backfill (run once)
   just bronze-upload-all 2026-01-01
   just silver-all 2026-01-01
   
   # Daily updates (run each day)
   just bronze-upload-all 2026-01-02
   just silver-all 2026-01-02
   ```

### Relevant Code

**Silver ETL deduplication** (`jobs/spark/silver_etl.py` lines 373-389):

```python
# Delete existing data for this course/ingest_date (idempotency)
spark.sql(f"""
    DELETE FROM {table} 
    WHERE course_id = '{args.course_id}' 
    AND ingest_date = '{args.ingest_date}'
""")

# Then append new data
valid.writeTo(table).append()
```

**Within-batch deduplication** (`jobs/spark/silver_etl.py` lines 327-335):

```python
# Dedup: prefer is_cache=true for same (round_id, fix_timestamp)
w = Window.partitionBy("round_id", "fix_timestamp").orderBy(
    F.col("is_cache").desc_nulls_last()
)
out = out.withColumn("_rn", F.row_number().over(w))
         .filter(F.col("_rn") == 1)
         .drop("_rn")
```

---

## Step 3: Silver Layer - Transform Data âœ…

> **Status**: âœ… Complete

### Command
```bash
# Process all courses at once
just silver-all

# Or process a single course
just silver <course_id> [date]

# Examples:
just silver indiancreek              # Process ALL dates for indiancreek
just silver indiancreek 2026-01-03   # Process only this specific date
just silver bradshawfarmgc           # Process ALL dates for bradshawfarmgc
```

- **No date**: Scans MinIO and processes ALL available dates for that course
- **With date**: Processes only that specific date

### What It Does
1. Reads Bronze files from MinIO
2. **Explodes** the `locations` array â†’ 1 row per location (so we can query/aggregate individual GPS fixes)
3. Writes to Iceberg table in Silver layer

### Files That Run
```
jobs/spark/silver_etl.py                # Main ETL script
  â”œâ”€â”€ detect_file_format()              # CSV or JSON?
  â”œâ”€â”€ discover_location_indices()       # Find location columns (CSV)
  â”œâ”€â”€ process CSV or JSON               # Different paths
  â””â”€â”€ write to Iceberg                  # Save to Silver

jobs/spark/lib/tm_lakehouse/
  â”œâ”€â”€ config.py                         # Configuration
  â”œâ”€â”€ iceberg.py                        # Iceberg helpers
  â””â”€â”€ spark_session.py                  # Spark setup
```

### Data Transformation
```
Bronze (1 row = 1 round with nested locations)
    â†“
Silver (1 row = 1 location record per round)

Example:
  Bronze: 1 row  â†’ round_123 with 55 nested locations
  Silver: 55 rows â†’ round_id=123, location_index=0, 1, 2, ... 54
```

### Key Silver Columns
| Column | Description |
|--------|-------------|
| `round_id` | Unique round identifier |
| `course_id` | Which golf course |
| `hole_number` | Current hole (1-18+) |
| `nine_number` | Which nine (1 or 2) |
| `pace_of_play` | Seconds for this section |
| `fix_timestamp` | When recorded |

### Our Silver Result
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ course_id       â”‚ rows    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bradshawfarmgc  â”‚ 466,634 â”‚
â”‚ erinhills       â”‚ 148,198 â”‚
â”‚ pinehurst4      â”‚ 137,837 â”‚
â”‚ americanfalls   â”‚  75,000 â”‚
â”‚ indiancreek     â”‚  67,660 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL           â”‚ 895,329 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How to Verify
```bash
# Check row counts by course
docker exec trino trino --execute "
  SELECT course_id, count(*) as rows 
  FROM iceberg.silver.fact_telemetry_event 
  GROUP BY course_id
"
```

---

## Step 4: Gold Layer - Analytics with dbt âœ…

> **Status**: âœ… Complete

### Command
```bash
just gold
```

### What It Does
1. Installs dbt-trino inside Airflow container
2. Runs dbt models that query Silver Iceberg tables via Trino
3. Creates pre-aggregated analytics tables in Gold schema

### Files That Run
```
transform/dbt_project/
  â”œâ”€â”€ dbt_project.yml             # dbt configuration
  â”œâ”€â”€ profiles.yml                # Trino connection settings
  â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ sources.yml             # Defines Silver as source
  â”‚   â””â”€â”€ gold/
  â”‚       â”œâ”€â”€ pace_summary_by_round.sql     # Round-level pace metrics
  â”‚       â”œâ”€â”€ signal_quality_rounds.sql     # Signal quality per round
  â”‚       â””â”€â”€ device_health_errors.sql      # Battery/health issues
  â””â”€â”€ packages.yml                # dbt_utils dependency

orchestration/airflow/dags/gold_dbt_dag.py  # Airflow DAG (70 lines)
```

### Gold Models

| Model | Description | Use Case |
|-------|-------------|----------|
| `pace_summary_by_round` | Round-level pace aggregates | "Which rounds were slow?" |
| `signal_quality_rounds` | Projected/problem fix rates | "Which rounds had GPS issues?" |
| `device_health_errors` | Low battery events | "Which devices need charging?" |

### Our Gold Results

---

#### 1. Pace Summary by Round

**What it shows:** Average pace (in seconds) and round duration for each course.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ course_id       â”‚ rounds â”‚ avg_pace  â”‚ avg_dur_min â”‚ avg_fixes â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bradshawfarmgc  â”‚ 16,437 â”‚ +367.5s   â”‚ 153 min     â”‚ 57        â”‚
â”‚ erinhills       â”‚  3,471 â”‚  +62.4s   â”‚ 268 min     â”‚ 43        â”‚
â”‚ pinehurst4      â”‚  3,010 â”‚ -256.2s   â”‚ 263 min     â”‚ 46        â”‚
â”‚ americanfalls   â”‚  2,949 â”‚  -84.3s   â”‚ 162 min     â”‚ 25        â”‚
â”‚ indiancreek     â”‚  1,684 â”‚ +373.7s   â”‚ 174 min     â”‚ 40        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insights:**
- **Positive pace** = behind schedule, **Negative pace** = ahead of schedule
- **Erin Hills** has longest avg round (268 min / 4.5 hrs) - championship course
- **American Falls** (9-hole) has only 25 fixes/round vs 57 for Bradshaw Farm (27-hole)
- **Pinehurst 4** players are fastest (-256s ahead of goal time on average)

**SQL Query (run in DBeaver):**
```sql
-- Pace Summary with Round Duration
SELECT 
    course_id,
    count(*) as rounds,
    round(avg(avg_pace), 1) as avg_pace_sec,
    round(avg(date_diff('minute', round_start_ts, round_end_ts)), 0) as avg_duration_min,
    round(avg(fix_count), 0) as avg_fixes,
    round(min(avg_pace), 0) as fastest_pace,
    round(max(avg_pace), 0) as slowest_pace
FROM iceberg.gold.pace_summary_by_round
WHERE round_end_ts > round_start_ts
GROUP BY course_id
ORDER BY rounds DESC;
```

---

#### 2. Device Health Issues by Course

**What it shows:** Battery issues as percentage of each course's total GPS events.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ course_id     â”‚ total_events â”‚ health_issues â”‚ % with issues   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ americanfalls â”‚ 75,000       â”‚ 38,689        â”‚ 51.6% âš ï¸        â”‚
â”‚ pinehurst4    â”‚ 137,837      â”‚ 3,464         â”‚ 2.5%            â”‚
â”‚ erinhills     â”‚ 148,198      â”‚ 1,416         â”‚ 1.0%            â”‚
â”‚ indiancreek   â”‚ 67,660       â”‚ 327           â”‚ 0.5%            â”‚
â”‚ bradshawfarmgcâ”‚ 933,268      â”‚ 0             â”‚ 0.0% âœ…         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insights:**
- **American Falls has a serious battery problem!** 51.6% of events have low/critical battery
- **Bradshaw Farm** has zero battery issues - excellent device management
- This suggests American Falls devices may need replacement or more frequent charging

**SQL Query (run in DBeaver):**
```sql
-- Device Health as % of Course Events
WITH course_totals AS (
    SELECT course_id, count(*) as total_events
    FROM iceberg.silver.fact_telemetry_event
    GROUP BY course_id
),
health_issues AS (
    SELECT course_id, count(*) as health_events
    FROM iceberg.gold.device_health_errors
    GROUP BY course_id
)
SELECT 
    c.course_id,
    c.total_events,
    coalesce(h.health_events, 0) as health_issues,
    round(coalesce(h.health_events, 0) * 100.0 / c.total_events, 1) as pct_with_issues
FROM course_totals c
LEFT JOIN health_issues h ON c.course_id = h.course_id
ORDER BY pct_with_issues DESC;
```

---

#### 3. Battery Issue Breakdown

**What it shows:** Critical vs Low battery events across all courses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ course_id     â”‚ health_flag      â”‚ events â”‚ % total â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ americanfalls â”‚ battery_critical â”‚ 38,573 â”‚  87.9%  â”‚
â”‚ pinehurst4    â”‚ battery_critical â”‚  1,799 â”‚   4.1%  â”‚
â”‚ pinehurst4    â”‚ battery_low      â”‚  1,665 â”‚   3.8%  â”‚
â”‚ erinhills     â”‚ battery_critical â”‚    802 â”‚   1.8%  â”‚
â”‚ erinhills     â”‚ battery_low      â”‚    614 â”‚   1.4%  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insights:**
- **87.9%** of ALL battery issues across ALL courses are from American Falls
- Most issues are `battery_critical` (<10%) rather than `battery_low` (<20%)

**SQL Query (run in DBeaver):**
```sql
-- Battery Issues by Type and Course
SELECT 
    course_id, 
    health_flag, 
    count(*) as events,
    round(count(*) * 100.0 / sum(count(*)) over(), 1) as pct_of_total
FROM iceberg.gold.device_health_errors
GROUP BY course_id, health_flag
ORDER BY events DESC;
```

### How to Verify
```bash
# List Gold tables
just trino-query "SHOW TABLES IN iceberg.gold"

# Query pace summary
just trino-query "SELECT * FROM iceberg.gold.pace_summary_by_round LIMIT 5"

# Check device health
just trino-query "SELECT health_flag, count(*) FROM iceberg.gold.device_health_errors GROUP BY 1"
```

### Data Flow
```
Silver (iceberg.silver.fact_telemetry_event)
    â†“
dbt models (SQL transformations)
    â†“
Gold (iceberg.gold.*)
    â”œâ”€â”€ pace_summary_by_round      # 27,551 rows
    â”œâ”€â”€ signal_quality_rounds      # 27,551 rows
    â””â”€â”€ device_health_errors       # 43,896 rows
```

---

## Step 5: Query Results âœ…

> **Status**: âœ… Complete

### Commands
```bash
# Interactive Trino shell
just trino

# Quick query
just trino-query "SELECT count(*) FROM iceberg.silver.fact_telemetry_event"

# Check Trino health
just trino-status
```

### Example Queries

**Silver Layer - Detailed Event Data**
```sql
-- Count rounds per course
SELECT course_id, count(DISTINCT round_id) as rounds
FROM iceberg.silver.fact_telemetry_event
GROUP BY course_id;

-- Average pace by hole at Bradshaw Farm
SELECT hole_number, nine_number, round(avg(pace), 0) as avg_pace
FROM iceberg.silver.fact_telemetry_event
WHERE course_id = 'bradshawfarmgc'
GROUP BY hole_number, nine_number
ORDER BY nine_number, hole_number;

-- Which nines are most popular at 27-hole course?
SELECT 
    array_agg(DISTINCT nine_number ORDER BY nine_number) as nines_played,
    count(DISTINCT round_id) as rounds
FROM iceberg.silver.fact_telemetry_event
WHERE course_id = 'bradshawfarmgc'
GROUP BY round_id
HAVING count(DISTINCT nine_number) >= 2;
```

**Gold Layer - Pre-Aggregated Analytics**
```sql
-- Slowest courses by average pace
SELECT course_id, round(avg(avg_pace), 0) as overall_pace
FROM iceberg.gold.pace_summary_by_round
GROUP BY course_id
ORDER BY overall_pace DESC;

-- Courses with battery issues
SELECT course_id, health_flag, count(*) as events
FROM iceberg.gold.device_health_errors
GROUP BY course_id, health_flag
ORDER BY events DESC;
```

### Our Query Results
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer           â”‚ Rows   â”‚ Description â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Silver          â”‚ 1.36M  â”‚ GPS events  â”‚
â”‚ Gold (pace)     â”‚ 27,551 â”‚ Round stats â”‚
â”‚ Gold (quality)  â”‚ 27,551 â”‚ Signal QA   â”‚
â”‚ Gold (health)   â”‚ 43,896 â”‚ Battery low â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Troubleshooting

### Check Container Logs
```bash
docker logs airflow --tail 50
docker logs spark --tail 50
```

### Restart a Service
```bash
just restart-service airflow
```

### Start Fresh (keep data)
```bash
just down
just up
```

### Nuclear Option (delete ALL data)
```bash
just nuke      # Stops everything, deletes all volumes
just up        # Start fresh with empty databases
```

### DAGs Not Running?
DAGs now start **unpaused** by default. If you have issues:
```bash
# Check DAG status
docker exec airflow airflow dags list

# Manually unpause if needed
docker exec airflow airflow dags unpause bronze_ingest
```

---

## Progress Tracker

| Step | Status | Notes |
|------|--------|-------|
| 1. Start Stack | âœ… Complete | All services running |
| 2. Bronze Upload | âœ… Complete | 7 files â†’ MinIO |
| 3. Silver ETL | âœ… Complete | 1.36M rows â†’ Iceberg |
| 4. Gold dbt | âœ… Complete | 3 analytics tables created |
| 5. Query Results | âœ… Complete | Trino queries working |

---

## What's Next?

The pipeline is complete! For production use:

1. **Backfill Historical Data**: Use `just backfill-silver` for resumable bulk processing
2. **Daily Operations**: Run `just pipeline-all` for full Bronze â†’ Silver â†’ Gold
3. **Monitoring**: Check `just backfill-status` for ingestion tracking

ğŸ“š **See also:** [Command Reference](command_reference.md) - Complete list of all `just` commands

---

*Last updated: All 5 steps complete - Full pipeline working with Bronze, Silver, and Gold layers*

